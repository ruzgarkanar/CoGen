# Model Configuration
MODEL_NAME=distilbert-base-uncased-distilled-squad
ENCODER_MODEL=sentence-transformers/all-mpnet-base-v2
HF_TOKEN = ""

# Training Parameters
LEARNING_RATE=5e-6
NUM_EPOCHS=50
TRAIN_BATCH_SIZE=1
MAX_LENGTH=512
WARMUP_RATIO=0.1
MAX_GRAD_NORM=0.1
EARLY_STOPPING_PATIENCE=5

# Cache Configuration
ENABLE_CACHE=true
CACHE_EXPIRY=3600
MAX_CACHE_ITEMS=1000

# Redis Configuration
REDIS_HOST=""
REDIS_PORT=""
REDIS_DB=0
REDIS_PASSWORD=""
REDIS_USERNAME=""

# Batch Processing
BATCH_SIZE=64
VECTOR_DIMENSION=768 
NUM_WORKERS=4

# Model Performance
DEVICE=cpu 
TOP_K_RETRIEVAL=3
SIMILARITY_THRESHOLD=0.3
CONFIDENCE_THRESHOLD=0.5
